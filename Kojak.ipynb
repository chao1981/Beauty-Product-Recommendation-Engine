{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from bson.son import SON\n",
    "from bson.code import Code\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "# These are packages need for natural language processing:\n",
    "import nltk\n",
    "from __future__ import division, unicode_literals \n",
    "from textblob import TextBlob as tb\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "#creating mbf database (mbf = \"meta beauty file\")\n",
    "mbf = client.dsbc.mbf\n",
    "# mbf.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('meta_Beauty.json') as meta_beauty_file:\n",
    "    for line in meta_beauty_file:\n",
    "        meta_f = json.loads(line)  \n",
    "        #only work when using json.loads() not json.load() \n",
    "        mbf.insert(meta_f)\n",
    "        \n",
    "#can't do json.load(meta_beautfy_file) since meta_beauty_file has the form \"{...}{...}{...}\" --> not json\n",
    "#json.load(line) does not work since line is  string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"categories\": [[\"Beauty\", \"Skin Care\",...]] --> need to $unwind\n",
    "# use \"categories.0\" get the innner list.\n",
    "\n",
    "#Skin_care = mbf.find({\"categories.0\": [\"Beauty\"]})\n",
    "\n",
    "pipeline  = [{\"$unwind\":\"$categories\"},\n",
    "             {\"$match\":{\"categories\":{\"$all\":[\"Face\",\"Creams & Moisturizers\"]}}},\n",
    "             {\"$project\":{\"_id\":0,\"asin\":1,\"title\":1,\"price\":1}},\n",
    "             {\"$out\": \"product_info\"}]\n",
    "# \"$out\" write the result into new collection\n",
    "# \"$all\" find all list that contains everything in queries\n",
    "mbf.aggregate(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "product_info = client.dsbc.product_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# name = product_info.find({}, {\"_id\":0,\"asin\":1})\n",
    "# id_list  = {}\n",
    "# for id_num in name:\n",
    "#     id_list.append(id_num[\"asin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = product_info.find({}, {\"_id\":0,\"asin\":1,\"title\":1,\"price\":1})\n",
    "id_list  = {}\n",
    "for id_num in name:\n",
    "    try:\n",
    "        id_list[id_num[\"asin\"]]=[id_num[\"title\"],id_num[\"price\"]]\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#id_list.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating database for reviews beauty file\n",
    "rbf = client.dsbc.rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('reviews_Beauty.json') as reviews_beauty_file:\n",
    "    for line in reviews_beauty_file:\n",
    "        review = json.loads(line)  \n",
    "        rbf.insert(review)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#new database for selective items\n",
    "ndb = client.dsbc.ndb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beauty_item_tb = {id_list.keys()[0] : id_list.values()[0], \n",
    "                    \"Reviews\" : [], \"rating\":{\"Five_star\":0, \n",
    "                                              \"Four_star\":0,\n",
    "                                              \"Three_star\":0,\n",
    "                                              \"Two_star\":0,\n",
    "                                              \"One_star\":0}}\n",
    "match_list = rbf.find({\"asin\":{\"$in\": [id_list.keys()[0]]}})\n",
    "\n",
    "for i in match_list:\n",
    "    beauty_item_tb[\"Reviews\"].append(tb(i['reviewText']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TF-IDF method\n",
    "\n",
    "tf(word, blob) computes \"term frequency\" which is the number of times a word appears in a document blob, normalized by dividing by the total number of words in blob. We use TextBlob for breaking up the text into words and getting the word counts.\n",
    "\n",
    "n_containing(word, bloblist) returns the number of documents containing word. A generator expression is passed to the sum() function.\n",
    "\n",
    "idf(word, bloblist) computes \"inverse document frequency\" which measures how common a word is among all documents in bloblist. The more common a word is, the lower its idf. We take the ratio of the total number of documents to the number of documents containing word, then take the log of that. Add 1 to the divisor to prevent division by zero.\n",
    "tfidf(word, blob, bloblist) computes the TF-IDF score. It is simply the product of tf and idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Sentiment(polarity=0.3722222222222222, subjectivity=0.6432098765432098)\n",
      "1 Sentiment(polarity=0.2, subjectivity=0.6)\n"
     ]
    }
   ],
   "source": [
    "for i, blob in enumerate(beauty_item_info['Reviews']):\n",
    "    print i, blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextBlob(\"I just started using this, so I'm not sure yet of the long-term effects, but overall it's a great moisturizer.  I'm fairly picky about moisturizers, since I tend to be really oily (my whole face, too...lucky me).  This one seems like it might be a winner.  It absorbed nicely and my skin did look and feel great after using it. It did lose one star, though, because of the price- it's a bit steep.\"),\n",
       " TextBlob(\"I bought it for a collagen moisturizer and immediately found it cleared my breakouts. I still use an under eye cream since I am 56 but this cream is miraculous. It soaks in so fast you can't believe it and it not greasy or shiny for someone with oily skin.\")]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beauty_item_info['Reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tWord: winner, TF-IDF: 0.0\n",
      "\tWord: just, TF-IDF: 0.0\n",
      "\tWord: feel, TF-IDF: 0.0\n",
      "\tWord: look, TF-IDF: 0.0\n",
      "\tWord: because, TF-IDF: 0.0\n",
      "\tWord: star, TF-IDF: 0.0\n",
      "\tWord: tend, TF-IDF: 0.0\n",
      "\tWord: really, TF-IDF: 0.0\n",
      "\tWord: to, TF-IDF: 0.0\n",
      "\tWord: seems, TF-IDF: 0.0\n",
      "Top words in document 2\n",
      "\tWord: breakouts, TF-IDF: 0.0\n",
      "\tWord: am, TF-IDF: 0.0\n",
      "\tWord: soaks, TF-IDF: 0.0\n",
      "\tWord: someone, TF-IDF: 0.0\n",
      "\tWord: still, TF-IDF: 0.0\n",
      "\tWord: cream, TF-IDF: 0.0\n",
      "\tWord: believe, TF-IDF: 0.0\n",
      "\tWord: eye, TF-IDF: 0.0\n",
      "\tWord: for, TF-IDF: 0.0\n",
      "\tWord: fast, TF-IDF: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i, blob in enumerate(beauty_item_tb['Reviews']):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, beauty_item_tb['Reviews']) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:10]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Based on the above analysis, we can see that it's not very helpful to look at TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beauty_item_info = {id_list.keys()[100] : id_list.values()[100], \n",
    "                    \"Reviews\" : [], \"rating\":{\"Five_star\":0, \n",
    "                                              \"Four_star\":0,\n",
    "                                              \"Three_star\":0,\n",
    "                                              \"Two_star\":0,\n",
    "                                              \"One_star\":0}}\n",
    "match_list = rbf.find({\"asin\":{\"$in\": [id_list.keys()[100]]}})\n",
    "\n",
    "for i in match_list:\n",
    "    beauty_item_info[\"Reviews\"].append(i['reviewText'])\n",
    "    if i['overall'] == 5.0:\n",
    "        beauty_item_info[\"rating\"][\"Five_star\"] += 1\n",
    "    elif i['overall'] == 4.0:\n",
    "        beauty_item_info[\"rating\"][\"Four_star\"] += 1\n",
    "    elif i['overall'] == 3.0:\n",
    "        beauty_item_info[\"rating\"][\"Three_star\"] += 1\n",
    "    elif i['overall'] == 2.0:\n",
    "        beauty_item_info[\"rating\"][\"Two_star\"] += 1\n",
    "    else:\n",
    "        beauty_item_info[\"rating\"][\"One_star\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Great service, my order arrived earlier than the expected 7-10 day shipping through USPS. I have used this product before and love it. I was happy to find this great deal through Amazon. I ordered two of these moisturizers and the Decleor boxes that the moisturizers came in were a little dinged up after making their way through US mail, but the jars inside were not damaged at all, so  I have no complaints. Would recommend this seller!The pros: You use a very tiny little bit, and it goes a long way. Feels very non-greasy but still quenches my skin's thirst.The con: Price. Although obviously you could do worse, this does cost more than other brands.All in all, I love this moisturizer and have been using it for years.I'm fussy about what I put on my 47 year old face these days. Luckily good family genes help me to look younger than my years, but I still moisturize and use sun screen. Being older and wiser helps me not to foolishly spend $ on face cream. I don't need or use 5 different ones or one for every problem. But this Hydra Floral cream is the best for me! Love the scent and best of all it's effective and a little goes a long way. A 1.7oz jar can last me a year. I ordered last time from 123Skincare, and I promised to give them a good review after a bumpy start to my order that they quickly replied to.  They were very interested in me being happy, and because of that I will order from them again!This cream is such a total waste of money. Your better off at the drug store!Sad but true, labeling is such a market strategy.I would give this 0 stars if I could because I ordered 'Hydra Floral Anti-Pollution&#34; but I received the &#34;Hydra Floral Multi-Protection&#34; face creme which is much thicker and has a completely different scent. It also does not moisturize my face as well, despite being thicker.I absolutely love the Hydra Floral face creme and have been using it for years - and have even ordered it from Amazon before without issue - so imagine my shock and disappointment when I received the wrong face creme and then found that Amazon does not allow this type of product to be returned. I understand that the small print of the product description says &#34;(Due to a manufacturer packaging change, item received may vary from photograph.)&#34; but IMO that should not mean that they can send you a completely different product than the one advertised and ordered. A different box or jar, sure, but not a completely DIFFERENT item. Very disappointed and won't be ordering this from Amazon again.\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ''\n",
    "for i in beauty_item_info['Reviews']:\n",
    "    text += i\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 100 #number of words to consider\n",
    "Cluster_threshold = 5 #distance between two words to consider\n",
    "Top_sentences = 5 #number of sentences to return for a \"top n\" summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _score_sentences(sentences, important_words,Cluster_threshold = 5):\n",
    "    scores = []\n",
    "    sentence_idx = -1\n",
    "    \n",
    "    #word_tokenize(s) split a sentence and find all the words and punctuations\n",
    "    for s in [nltk.tokenize.word_tokenize(s) for s in sentences]:\n",
    "        sentence_idx += 1\n",
    "        word_idx =[]\n",
    "        \n",
    "        # This function check whether any important words occur in the sentence\n",
    "        # if  there is, record the location of the important word in the sentence\n",
    "        for w in important_words:\n",
    "            try:\n",
    "                word_idx.append(s.index(w))\n",
    "            except ValueError, e:\n",
    "                pass\n",
    "        word_idx.sort()\n",
    "        \n",
    "        #If there is no important words in the sentence, skip the rest of the loop\n",
    "        #and goes on to next sentence\n",
    "        if len(word_idx) == 0 : continue\n",
    "        \n",
    "        #compute clusters by using a max distance threshold for any two consecutive words\n",
    "        clusters = []\n",
    "        cluster = [word_idx[0]]\n",
    "        \n",
    "        i = 1\n",
    "        while i < len(word_idx):\n",
    "            if word_idx[i] - word_idx[i-1] < Cluster_threshold:\n",
    "                cluster.append(word_idx[i])\n",
    "            else:\n",
    "                clusters.append(cluster[:])\n",
    "                cluster = [word_idx[i]]\n",
    "            i += 1\n",
    "        clusters.append(cluster)\n",
    "        \n",
    "        #now score each cluster. The max score for any give cluster is the score for the \n",
    "        #sentence\n",
    "        \n",
    "        max_cluster_score = 0\n",
    "        for c in clusters:\n",
    "            num_important_words_in_cluster = len(c)\n",
    "            total_words_in_cluster = c[-1] - c[0] + 1\n",
    "            score = 1.0 * num_important_words_in_cluster\\\n",
    "                    *(num_important_words_in_cluster/total_words_in_cluster)\n",
    "            \n",
    "            if score > max_cluster_score:\n",
    "                max_cluster_score = score \n",
    "        scores.append((sentence_idx,score))\n",
    "    return scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def summarize(txt, N=100, Top_sentences = 5):\n",
    "    sentences = [s.lower() for s in nltk.tokenize.sent_tokenize(txt)]\n",
    "    \n",
    "    words = [w.lower() for sentence in sentences for w in nltk.tokenize.word_tokenize(sentence)]\n",
    "    fqdist = nltk.FreqDist(words)\n",
    "    \n",
    "    stop_words = nltk.corpus.stopwords.words('english') + ['usps','shipping']\n",
    "    \n",
    "    top_n_words = [w[0] for w in fqdist.items() \n",
    "                   if w[0] not in stop_words][:N]\n",
    "    \n",
    "    scored_sentences = _score_sentences(sentences, top_n_words)\n",
    "    \n",
    "    #Summarization Approach 1:\n",
    "    avg = np.mean([s[1] for s in scored_sentences])\n",
    "    std = np.std([s[1] for s in scored_sentences])\n",
    "    mean_scored = [(sent_idx, score) for (sent_idx, score) in scored_sentences \n",
    "                    if score > avg + 0.5 *std ]\n",
    "    \n",
    "    #Summarization Approach 2:\n",
    "    top_n_scored = sorted(scored_sentences, key=lambda s: s[1])[-Top_sentences:]\n",
    "    top_n_scored = sorted(top_n_scored, key=lambda s: s[0])\n",
    "    \n",
    "    return dict(top_n_summary=[sentences[idx] for (idx,score) in top_n_scored],\n",
    "                mean_score_summary=[sentences[idx] for (idx,score) in mean_scored])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"feels very non-greasy but still quenches my skin's thirst.the con: price.\",\n",
       " u'luckily good family genes help me to look younger than my years, but i still moisturize and use sun screen.',\n",
       " u'being older and wiser helps me not to foolishly spend $ on face cream.',\n",
       " u'i understand that the small print of the product description says &#34;(due to a manufacturer packaging change, item received may vary from photograph.']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(text)['mean_score_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"feels very non-greasy but still quenches my skin's thirst.the con: price.\",\n",
       " u'luckily good family genes help me to look younger than my years, but i still moisturize and use sun screen.',\n",
       " u'being older and wiser helps me not to foolishly spend $ on face cream.',\n",
       " u'a 1.7oz jar can last me a year.',\n",
       " u'i understand that the small print of the product description says &#34;(due to a manufacturer packaging change, item received may vary from photograph.']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(text)['top_n_summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###We can see that document summarization provides much more informations that are important about a product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RATING ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def amazon_rating(x1,x2,x3,x4,x5):\n",
    "    return (x1+2*x2+3*x3+4*x4+5*x5)/float(x1+x2+x3+x4+x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 shades rating 3.47656102918\n",
      "twightlight rating 4.12725696952\n",
      "the hobbit rating 4.69792682927\n",
      "daughter of the forest 4.47708333333\n",
      "best seller of 2014 4.41966048164\n",
      "divergent 4.52524465651\n",
      "eragon 3.95181718062\n",
      "to kill a mocking bird 4.67559626685\n"
     ]
    }
   ],
   "source": [
    "print \"50 shades rating\", amazon_rating(7708,2732,2970,3584,14876)\n",
    "print \"twightlight rating\", amazon_rating(802,349,503,781,4488)\n",
    "print \"the hobbit rating\", amazon_rating(141,104,265,1071,6619)\n",
    "print \"daughter of the forest\", amazon_rating(20,20,19,73,348)\n",
    "print \"best seller of 2014\", amazon_rating(100,108,166,414,1745)\n",
    "print \"divergent\", amazon_rating(411,519,1316,3918,14375)\n",
    "print \"eragon\", amazon_rating(443,267,331,572,2019)\n",
    "print \"to kill a mocking bird\", amazon_rating(141,96,184,657,4708)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smooth_rating(x1,x2,x3,x4,x5,alpha,beta):\n",
    "    return ((x1+2*x2+3*x3+4*x4+5*x5)+alpha*beta)/(float(x1+x2+x3+x4+x5)+beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 shades rating 3.47507037848\n",
      "twightlight rating 4.11120603731\n",
      "the hobbit rating 4.67746987952\n",
      "daughter of the forest 4.2224137931\n",
      "best seller of 2014 4.36574249905\n",
      "divergent 4.51785454722\n",
      "eragon 3.92631296892\n",
      "to kill a mocking bird 4.64712878016\n"
     ]
    }
   ],
   "source": [
    "print \"50 shades rating\", smooth_rating(7708,2732,2970,3584,14876,3,100)\n",
    "print \"twightlight rating\", smooth_rating(802,349,503,781,4488,3,100)\n",
    "print \"the hobbit rating\", smooth_rating(141,104,265,1071,6619,3,100)\n",
    "print \"daughter of the forest\", smooth_rating(20,20,19,73,348,3,100)\n",
    "print \"best seller of 2014\", smooth_rating(100,108,166,414,1745,3,100)\n",
    "print \"divergent\", smooth_rating(411,519,1316,3918,14375,3,100)\n",
    "print \"eragon\", smooth_rating(443,267,331,572,2019,3,100)\n",
    "print \"to kill a mocking bird\", smooth_rating(141,96,184,657,4708,3,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_extreme_rating(x1,x2,x3,x4,x5,alpha,beta):\n",
    "    numer = (2*x1+2*x2+3*x3+4*x4+10*x5)+alpha*beta\n",
    "    denom = float(2*x1+x2+x3+x4+2*x5+beta)\n",
    "    return numer/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 shades rating 3.54118854713\n",
      "twightlight rating 4.23251847641\n",
      "the hobbit rating 4.78479415671\n",
      "daughter of the forest 4.43987341772\n",
      "best seller of 2014 4.53774006253\n",
      "divergent 4.67268877911\n",
      "eragon 4.06700032289\n",
      "to kill a mocking bird 4.75398230088\n"
     ]
    }
   ],
   "source": [
    "print \"50 shades rating\", weight_extreme_rating(7708,2732,2970,3584,14876,3,100)\n",
    "print \"twightlight rating\", weight_extreme_rating(802,349,503,781,4488,3,100)\n",
    "print \"the hobbit rating\", weight_extreme_rating(141,104,265,1071,6619,3,100)\n",
    "print \"daughter of the forest\", weight_extreme_rating(20,20,19,73,348,3,100)\n",
    "print \"best seller of 2014\", weight_extreme_rating(100,108,166,414,1745,3,100)\n",
    "print \"divergent\", weight_extreme_rating(411,519,1316,3918,14375,3,100)\n",
    "print \"eragon\", weight_extreme_rating(443,267,331,572,2019,3,100)\n",
    "print \"to kill a mocking bird\", weight_extreme_rating(141,96,184,657,4708,3,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strict_rating(x1,x2,x3,x4,x5,alpha,beta):\n",
    "    numer = (2*x1+4*x2+3*x3+4*x4+5*x5)+alpha*beta\n",
    "    denom = float(2*x1+x2+x3+x4+x5+beta)\n",
    "    return numer/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 shades rating 2.7851686152\n",
      "twightlight rating 3.61365480468\n",
      "the hobbit rating 4.58086693079\n",
      "daughter of the forest 4.07903225806\n",
      "best seller of 2014 4.20437698553\n",
      "divergent 4.43148035972\n",
      "eragon 3.48051104374\n",
      "to kill a mocking bird 4.51151102464\n"
     ]
    }
   ],
   "source": [
    "print \"50 shades rating\", strict_rating(7708,2732,2970,3584,14876,3,100)\n",
    "print \"twightlight rating\", strict_rating(802,349,503,781,4488,3,100)\n",
    "print \"the hobbit rating\", strict_rating(141,104,265,1071,6619,3,100)\n",
    "print \"daughter of the forest\", strict_rating(20,20,19,73,348,3,100)\n",
    "print \"best seller of 2014\", strict_rating(100,108,166,414,1745,3,100)\n",
    "print \"divergent\", strict_rating(411,519,1316,3918,14375,3,100)\n",
    "print \"eragon\", strict_rating(443,267,331,572,2019,3,100)\n",
    "print \"to kill a mocking bird\", strict_rating(141,96,184,657,4708,3,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.06906906907\n",
      "3.77725118483\n",
      "4.19783197832\n",
      "3.52302631579\n",
      "3.42258748674\n"
     ]
    }
   ],
   "source": [
    "print strict_rating(1,0,3,32,193,2.5,100)\n",
    "print strict_rating(0,0,1,6,104,2.5,100)\n",
    "print strict_rating(0,1,7,31,230,2.5,100)\n",
    "print strict_rating(253,184,297,590,2428,2.5,100)\n",
    "print strict_rating(101,64,117,205,895,2.5,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strict_rating(x1,x2,x3,x4,x5,alpha,beta):\n",
    "    numer = (3*x1+4*x2+3*x3+4*x4+5*x5)+alpha*beta\n",
    "    denom = float(5*x1+x2+x3+x4+x5+beta)\n",
    "    return numer/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.stem.snowball.SnowballStemmer at 0x1176bd410>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
